# slurm_benchmark

Simple helpers for sweeping parameter combinations on a Slurm cluster and
collecting accounting data.

## Quick start

```python
from slurm_benchmark import SlurmBenchmark

benchmark = SlurmBenchmark(
    command_template=[
        "python",
        "train.py",
        "--dataset",
        "{dataset}",
        "--k",
        "{k}",
    ],
    parameters={
        "dataset": ["mnist", "cifar10"],
        "k": [3, 5, 10],
    },
    resources={
        "--mem": ["1G", "2G", "4G"],
        "--cpus-per-task": [1, 2, 4],
    },
    delay=2.0,
)

results = benchmark.run()
print(results)
```

Each job submission is generated by filling in the placeholders in
`command_template` with the entries of `parameters`. Resource options are passed
verbatim to ``sbatch`` and the job is wrapped using ``--wrap``. Job accounting
information is retrieved with ``sacct`` and stored alongside the parameter and
resource selections in a pandas ``DataFrame``.
